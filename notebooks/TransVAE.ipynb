{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from psmiles import PolymerSmiles as PS\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from PIL import Image\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "import atomInSmiles\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output, display\n",
    "import ipywidgets as widgets\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "class load_data(Dataset):\n",
    "    def __init__(self, path):\n",
    "        #csv 읽기\n",
    "        self.raw = pd.read_csv(path)\n",
    "\n",
    "        #SMILES\n",
    "        self.SMILES = self.raw.iloc[:, 1:2].values\n",
    "        self.SMILES = np.squeeze(self.SMILES)\n",
    "\n",
    "        #특성 데이터\n",
    "        self.properties = self.raw.iloc[:, 6:11].values\n",
    "        self.properties = np.squeeze(self.properties)\n",
    "\n",
    "        #Degree of Polymerization\n",
    "        self.DP = self.raw.iloc[:, 4:5].values\n",
    "        self.DP = np.squeeze(self.DP)\n",
    "\n",
    "        #PSMILES 변환\n",
    "        psmiles = []\n",
    "        for smiles in self.SMILES:\n",
    "            ps = PS(smiles)\n",
    "            ps.canonicalize\n",
    "            psmiles.append(ps.psmiles)\n",
    "\n",
    "        #Atom-In-SMILES Encoding\n",
    "        ais_encoding = []\n",
    "        for smiles in psmiles:\n",
    "            ais_encoding.append(atomInSmiles.encode(smiles))\n",
    "        \n",
    "\n",
    "\n",
    "        #Atom-In-SMILES Tokenization (Encoder)\n",
    "        ais_tokens = []\n",
    "        for smiles in ais_encoding:\n",
    "            ais_tokens.append(atomInSmiles.smiles_tokenizer(\"[SOS] \" + smiles +\" [EOS]\"))\n",
    "\n",
    "        max_len = len(max(ais_encoding, key=len))\n",
    "        self.max_len = max_len\n",
    "        print(\"max sequence length : \", max_len)\n",
    "\n",
    "        #vocab 구성\n",
    "        corpus = []\n",
    "        for frags in ais_tokens:\n",
    "            corpus.extend(frags)\n",
    "        token_count = Counter(corpus)\n",
    "        vocab = { token:i for i, (token, count) in enumerate(sorted(token_count.items(), key=lambda x: x[1], reverse=True))}\n",
    "        vocab_size = len(vocab)\n",
    "\n",
    "        num_data = len(ais_tokens)\n",
    "        print(vocab)\n",
    "\n",
    "        ais_tokens_enc = ais_tokens\n",
    "        ais_tokens_enc = [[tok for tok in tokens if tok not in ['[SOS]', '[EOS]']] for tokens in ais_tokens_enc]\n",
    "\n",
    "        ais_tokens_dec_input = ais_tokens\n",
    "        ais_tokens_dec_input = [[tok for tok in tokens if tok not in ['[EOS]']] for tokens in ais_tokens_dec_input]\n",
    "\n",
    "        ais_tokens_dec_output = ais_tokens\n",
    "        ais_tokens_dec_output = [[tok for tok in tokens if tok not in ['[SOS]']] for tokens in ais_tokens_dec_output]\n",
    "\n",
    "        #Tokens to number (encoder)\n",
    "        ais_token_num_enc = torch.zeros((num_data, max_len), dtype=torch.long)\n",
    "        i=0\n",
    "        for tokens in ais_tokens_enc:\n",
    "            for length in range((len(tokens))):\n",
    "                ais_token_num_enc[i, length] = vocab[tokens[length]]\n",
    "            i += 1\n",
    "\n",
    "        #Tokens to number (Decoder Input)\n",
    "        ais_token_num_dec_input = torch.zeros((num_data, max_len), dtype=torch.long)\n",
    "        i=0\n",
    "        for tokens in ais_tokens_dec_input:\n",
    "            for length in range((len(tokens))):\n",
    "                ais_token_num_dec_input[i, length] = vocab[tokens[length]]\n",
    "            i += 1\n",
    "\n",
    "        #Tokens to number (Decoder Output)\n",
    "        ais_token_num_dec_output = torch.zeros((num_data, max_len), dtype=torch.long)\n",
    "        i=0\n",
    "        for tokens in ais_tokens_dec_output:\n",
    "            for length in range((len(tokens))):\n",
    "                ais_token_num_dec_output[i, length] = vocab[tokens[length]]\n",
    "            i += 1\n",
    "\n",
    "        self.SMILES_enc = ais_token_num_enc\n",
    "        self.SMILES_dec_input = ais_token_num_dec_input\n",
    "        self.SMILES_dec_output = ais_token_num_dec_output\n",
    "\n",
    "        \n",
    "        # self.SMILES = torch.ones((num_data, max_len, vocab_size), dtype=torch.float) * 0.1\n",
    "        # for smiles in range(ais_token_num_enc.shape[0]):\n",
    "        #     for length in range(ais_token_num_enc.shape[1]):\n",
    "        #         if ais_token_num_enc[smiles, length] != 0:\n",
    "        #             self.SMILES[smiles, length, ais_token_num_enc[smiles, length]] = 0.9\n",
    "\n",
    "        # self.SMILES_dec_input = torch.ones((num_data, max_len, vocab_size), dtype=torch.float) * 0.1\n",
    "        # for smiles in range(ais_token_num_dec_input.shape[0]):\n",
    "        #     for length in range(ais_token_num_dec_input.shape[1]):\n",
    "        #         if ais_token_num_dec_input[smiles, length] != 0:\n",
    "        #             self.SMILES_dec_input[smiles, length, ais_token_num_dec_input[smiles, length]] = 0.9\n",
    "\n",
    "        # self.SMILES_dec_output = torch.zeros((num_data, max_len, vocab_size), dtype=torch.float) * 0.1\n",
    "        # for smiles in range(ais_token_num_dec_output.shape[0]):\n",
    "        #     for length in range(ais_token_num_dec_output.shape[1]):\n",
    "        #         if ais_token_num_dec_output[smiles, length] != 0:\n",
    "        #             self.SMILES_dec_output[smiles, length, ais_token_num_dec_output[smiles, length]] = 0.9\n",
    "        \n",
    "        # self.SMILES_enc = self.SMILES\n",
    "        \n",
    "        # print(self.SMILES.shape, self.SMILES_dec_input.shape, self.SMILES_dec_output.shape)\n",
    "        \n",
    "        vocab_size, num_data\n",
    "        print(\"vocab size : \", vocab_size,\"\\nnumber of data : \",num_data)\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        \n",
    "\n",
    "        #PCA\n",
    "        self.pca = PCA(n_components=1).fit_transform(self.properties[:, 0:4])\n",
    "        self.pca = torch.tensor(self.pca, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "        print(self.SMILES_enc.shape)\n",
    "        self.DP = torch.tensor(self.DP, dtype=torch.float).to(device)\n",
    "        self.properties = torch.tensor(self.properties[:, 4:5], dtype=torch.float).to(device)\n",
    "        self.properties = torch.cat((self.pca, self.properties, self.DP.unsqueeze(-1)), dim=-1).unsqueeze(-1)\n",
    "        print(self.properties.shape)\n",
    "\n",
    "        self.test_data = self.SMILES_enc[50]\n",
    "\n",
    "        print(\"PSMILES : \",psmiles[50])\n",
    "        print(\"After AIS encoding : \", ais_encoding[50])\n",
    "        print(\"After AIS Tokenization : \", ais_tokens_enc[50])\n",
    "        print(\"After to number : \", ais_token_num_enc[50])\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.SMILES_enc[i], self.SMILES_dec_input[i], self.SMILES_dec_output[i], self.properties[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.SMILES_enc.shape[0]\n",
    "    \n",
    "    def vocab_len(self):\n",
    "        return self.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sequence length :  264\n",
      "{'(': 0, ')': 1, '=': 2, '[O;!R;C]': 3, '[CH3;!R;C]': 4, '[CH2;!R;CN]': 5, '[CH2;!R;CC]': 6, '[CH2;!R;CO]': 7, '[*;!R;C]': 8, '[SOS]': 9, '[EOS]': 10, '[O;!R;CC]': 11, '[NH;!R;CC]': 12, '[*;!R;O]': 13, '[O;!R;*C]': 14, '[C;!R;*OO]': 15, '[CH;!R;CCO]': 16, '[CH;!R;CCN]': 17, '[C;!R;CNO]': 18, '[N;!R;CCC]': 19, '[C;!R;*NO]': 20, '[*;!R;N]': 21, '[NH;!R;*C]': 22, '[CH3;!R;N]': 23, '[CH;!R;CCC]': 24, '[C;!R;CCCO]': 25, '[CH;!R;CC]': 26, '[F;!R;C]': 27, '[CH2;!R;C]': 28, '[C;!R;COO]': 29, '[CH3;!R;O]': 30, '[C;!R;CCCN]': 31, '#': 32, '[C;!R;CCCC]': 33, '[CH2;!R;CS]': 34, '[C;!R;CC]': 35, '[OH;!R;C]': 36, '[S;!R;CC]': 37, '[N;!R;C]': 38, '[C;!R;CN]': 39, '[CH;!R;C]': 40, '[C;!R;CCC]': 41, '[NH2;!R;C]': 42, '[CH;!R;CFF]': 43, '[O;!R;S]': 44, '[O;!R;CN]': 45, '[CH;!R;CCS]': 46, '[CH2;!R;CF]': 47, '[CH3;!R;S]': 48, '[C;!R;CCO]': 49, '[NH;!R;CO]': 50, '[C;!R;NNO]': 51, '[CH2;!R;*C]': 52, '[C;!R;CFFF]': 53, '[C;!R;CCFF]': 54, '[C;!R;NOO]': 55, '[C;!R;OOO]': 56, '[S;!R;CCOO]': 57, '[CH3;!R;Si]': 58, '[C;!R;NNS]': 59, '[S;!R;C]': 60, '[Cl;!R;C]': 61, '[NH;!R;C]': 62, '[OH;!R;P]': 63, '1': 64, '[C;!R;CNN]': 65, '[CH;!R;CN]': 66, '[SH;!R;C]': 67, '[CH;!R;CCF]': 68, '[O;!R;P]': 69, '[CH2;!R;NO]': 70, '[C;!R;CCCl]': 71, '[CH;!R;NN]': 72, '[CH2;!R;CSi]': 73, '[O;R;CC]': 74, '[cH;R;CC]': 75, '[c;R;CCO]': 76, '[CH;!R;CO]': 77, '[Si;!R;CCCC]': 78, '[NH;!R;CS]': 79, '[N;!R;CN]': 80, '[C;!R;CCN]': 81, '[C;!R;CCNN]': 82, '[N;!R;CCO]': 83, '[CH;!R;COO]': 84, '[P;!R;COOO]': 85, '[N;!R;CO]': 86, '[CH;!R;CCl]': 87, '[S;!R;CNOO]': 88, '[C;!R;CCCF]': 89, '[CH2;!R;NSi]': 90, '[C;!R;CCOO]': 91, '[NH2;!R;O]': 92, '[C;!R;CCF]': 93, '[CH;R;CCO]': 94, '[CH2;R;CO]': 95, '[C;R;OOO]': 96, '[O;!R;CS]': 97, '[NH;!R;CN]': 98, '[OH;!R;N]': 99, '[C;!R;NNN]': 100, '[N;!R;CS]': 101, '[S;!R;CCNO]': 102, '[N;!R;CCN]': 103, '[S;!R;CCO]': 104, '[OH;!R;S]': 105, '[CH;!R;CNP]': 106, '[CH2;!R;CP]': 107, '[CH;!R;CS]': 108, '[S;!R;NNOO]': 109, '[NH2;!R;S]': 110, '[N;!R;CC]': 111, '[NH2;!R;N]': 112, '[Si;!R;*CCC]': 113, '[*;!R;Si]': 114, '2': 115, '[CH;R;*CN]': 116, '[N;R;CCC]': 117, '[C;!R;*CO]': 118, '[S;!R;COOO]': 119, '[O;!R;CP]': 120, '[C;!R;CCCS]': 121, '[C;!R;NN]': 122, '[CH2;!R;OO]': 123, '[CH2;!R;NS]': 124, '[S;!R;COOS]': 125, '[S;!R;S]': 126, 'N': 127, '[P;!R;OOOO]': 128, '[CH;!R;CNO]': 129, '[CH;!R;FFO]': 130, '[CH;!R;CNN]': 131, '[C;!R;CSi]': 132, '[c;R;CCC]': 133, '[c;R;*CC]': 134, '[C;R;NNO]': 135, '[S;!R;OOO]': 136, '[O;!R;*S]': 137, '[CH2;!R;SS]': 138, '[CH2;!R;OP]': 139, '[C;!R;CFFO]': 140, '[C;!R;NSS]': 141, '[CH;!R;COP]': 142, '[CH;!R;CNS]': 143, '[C;!R;CCCCl]': 144, '[CH;!R;*CC]': 145, '[CH2;!R;NN]': 146, '[N;!R;CCS]': 147, '[PH;!R;CCO]': 148, '[P;!R;CCOO]': 149, '[C;!R;COS]': 150, '[C;!R;CCClCl]': 151, '[C;!R;OSS]': 152, '[CH2;!R;OSi]': 153, '[C;!R;CCFO]': 154, '[C;!R;CClCl]': 155, '[[N+]': 156, 'O': 157, '[[O-]': 158, '[N;!R;NO]': 159, '[SH;!R;O]': 160, '[CH;!R;NO]': 161, '[C;!R;CCNO]': 162, '[CH;!R;NNN]': 163, '[S;!R;OOOO]': 164, '[C;!R;CCOP]': 165}\n",
      "vocab size :  166 \n",
      "number of data :  6270\n",
      "torch.Size([6270, 264])\n",
      "torch.Size([6270, 3, 1])\n",
      "PSMILES :  CC(CCCNC(=O)C(C)OC(=O)[*])O[*]\n",
      "After AIS encoding :  [*;!R;O] [O;!R;*C] [CH;!R;CCO] ( [CH3;!R;C] ) [CH2;!R;CC] [CH2;!R;CC] [CH2;!R;CN] [NH;!R;CC] [C;!R;CNO] ( = [O;!R;C] ) [CH;!R;CCO] ( [CH3;!R;C] ) [O;!R;CC] [C;!R;*OO] ( [*;!R;C] ) = [O;!R;C]\n",
      "After AIS Tokenization :  ['[*;!R;O]', '[O;!R;*C]', '[CH;!R;CCO]', '(', '[CH3;!R;C]', ')', '[CH2;!R;CC]', '[CH2;!R;CC]', '[CH2;!R;CN]', '[NH;!R;CC]', '[C;!R;CNO]', '(', '=', '[O;!R;C]', ')', '[CH;!R;CCO]', '(', '[CH3;!R;C]', ')', '[O;!R;CC]', '[C;!R;*OO]', '(', '[*;!R;C]', ')', '=', '[O;!R;C]']\n",
      "After to number :  tensor([13, 14, 16,  0,  4,  1,  6,  6,  5, 12, 18,  0,  2,  3,  1, 16,  0,  4,\n",
      "         1, 11, 15,  0,  8,  1,  2,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "Polymers = \"simulation-trajectory-aggregate.csv\"\n",
    "dataset = load_data(Polymers)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=256, shuffle=True, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#from torch_pca import PCA\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer\n",
    "from fast_transformers.masking import TriangularCausalMask\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased=False, keepdim=True)\n",
    "        # '-1' means last dimension. \n",
    "\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pos_embedding =  self.pe[:, :x.size(1), :]\n",
    "        pos_embedding = torch.repeat_interleave(pos_embedding, x.shape[0], dim=0)\n",
    "        x =  torch.cat([x, pos_embedding], dim=2)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TFEncoder(nn.Module):\n",
    "    def __init__(self, d_model=512, n_heads=4, d_ff=32, enc_seq_len=5000, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.normLayer_0 = LayerNorm(d_model=d_model)\n",
    "        self.normLayer_1 = LayerNorm(d_model=d_model // 2)\n",
    "        self.normLayer_2 = LayerNorm(d_model=d_model // 4)\n",
    "        self.normLayer_3 = LayerNorm(d_model=d_model // 8)\n",
    "        \n",
    "        self.encoderLayer_0 = TransformerEncoderLayer(batch_first=True,\n",
    "                                               d_model=d_model,\n",
    "                                               nhead=n_heads,\n",
    "                                               dim_feedforward=d_ff,\n",
    "                                               dropout=dropout,\n",
    "                                               activation=\"gelu\")\n",
    "        self.encoderLayer_1 = TransformerEncoderLayer(batch_first=True,\n",
    "                                               d_model=d_model // 2,\n",
    "                                               nhead=n_heads,\n",
    "                                               dim_feedforward=d_ff,\n",
    "                                               dropout=dropout,\n",
    "                                               activation=\"gelu\")\n",
    "        self.encoderLayer_2 = TransformerEncoderLayer(batch_first=True,\n",
    "                                               d_model=d_model // 4,\n",
    "                                               nhead=n_heads,\n",
    "                                               dim_feedforward=d_ff,\n",
    "                                               dropout=dropout,\n",
    "                                               activation=\"gelu\")\n",
    "        self.encoderLayer_3 = TransformerEncoderLayer(batch_first=True,\n",
    "                                               d_model=d_model // 8,\n",
    "                                               nhead=n_heads,\n",
    "                                               dim_feedforward=d_ff,\n",
    "                                               dropout=dropout,\n",
    "                                               activation=\"gelu\")\n",
    "        self.encoder_0 = TransformerEncoder(encoder_layer=self.encoderLayer_0, num_layers=1,\n",
    "                                          norm=self.normLayer_0)\n",
    "        self.encoder_1 = TransformerEncoder(encoder_layer=self.encoderLayer_1,num_layers=1,\n",
    "                                          norm=self.normLayer_1)\n",
    "        self.encoder_2 = TransformerEncoder(encoder_layer=self.encoderLayer_2,num_layers=1,\n",
    "                                          norm=self.normLayer_2)\n",
    "        self.encoder_3 = TransformerEncoder(encoder_layer=self.encoderLayer_3,num_layers=1,\n",
    "                                          norm=self.normLayer_3)\n",
    "        self.input_embedding_smiles = nn.Embedding(dataset.vocab_size, d_model // 2)\n",
    "        self.input_embedding = nn.Sequential(\n",
    "            nn.Linear(1, d_model // 8),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 8, d_model // 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 4, d_model // 2),\n",
    "        )\n",
    "        self.pos_encoding = PositionalEncoding(d_model // 2, dropout, max_len=enc_seq_len)\n",
    "\n",
    "        self.to_encoder_1 = nn.Conv1d(in_channels=d_model, out_channels=d_model // 2, kernel_size=1)\n",
    "        self.to_encoder_2 = nn.Conv1d(in_channels=d_model // 2, out_channels=d_model // 4, kernel_size=1)\n",
    "        self.to_encoder_3 = nn.Conv1d(in_channels=d_model // 4, out_channels=d_model // 8, kernel_size=1)\n",
    "\n",
    "    def forward(self, smiles_enc):\n",
    "        smiles_enc = self.input_embedding_smiles(smiles_enc)\n",
    "        enc_input_0 = self.pos_encoding(smiles_enc)\n",
    "\n",
    "        encoded_0 = self.encoder_0(enc_input_0)\n",
    "\n",
    "        enc_input_1 = self.to_encoder_1(encoded_0.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        encoded_1 = self.encoder_1(enc_input_1)\n",
    "    \n",
    "        enc_input_2 = self.to_encoder_2(encoded_1.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        encoded_2 = self.encoder_2(enc_input_2)\n",
    "\n",
    "        enc_input_3 = self.to_encoder_3(encoded_2.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        encoded_3 = self.encoder_3(enc_input_3)\n",
    "        return encoded_3\n",
    "\n",
    "class TFDecoder(nn.Module):\n",
    "    def __init__(self, d_model=512, n_heads=4, d_ff=32, enc_seq_len=5000, dropout=0.2, ):\n",
    "        super().__init__()\n",
    "        self.normLayer_0 = LayerNorm(d_model=d_model // 8)\n",
    "        self.normLayer_1 = LayerNorm(d_model=d_model // 4)\n",
    "        self.normLayer_2 = LayerNorm(d_model=d_model // 2)\n",
    "        self.normLayer_3 = LayerNorm(d_model=d_model)\n",
    "\n",
    "        \n",
    "        self.decoderLayer_0 = TransformerDecoderLayer(batch_first=True,\n",
    "                                               d_model=d_model // 8,\n",
    "                                               nhead=n_heads,\n",
    "                                               dim_feedforward=d_ff,\n",
    "                                               dropout=dropout,\n",
    "                                               activation=\"gelu\")\n",
    "        self.decoderLayer_1 = TransformerDecoderLayer(batch_first=True,\n",
    "                                               d_model=d_model // 4,\n",
    "                                               nhead=n_heads,\n",
    "                                               dim_feedforward=d_ff,\n",
    "                                               dropout=dropout,\n",
    "                                               activation=\"gelu\")\n",
    "        self.decoderLayer_2 = TransformerDecoderLayer(batch_first=True,\n",
    "                                               d_model=d_model // 2,\n",
    "                                               nhead=n_heads,\n",
    "                                               dim_feedforward=d_ff,\n",
    "                                               dropout=dropout,\n",
    "                                               activation=\"gelu\")\n",
    "        self.decoderLayer_3 = TransformerDecoderLayer(batch_first=True,\n",
    "                                               d_model=d_model,\n",
    "                                               nhead=n_heads,\n",
    "                                               dim_feedforward=d_ff,\n",
    "                                               dropout=dropout,\n",
    "                                               activation=\"gelu\")\n",
    "        self.decoder_0 = TransformerDecoder(decoder_layer=self.decoderLayer_0,num_layers=1,\n",
    "                                          norm=self.normLayer_0)\n",
    "        self.decoder_1 = TransformerDecoder(decoder_layer=self.decoderLayer_1,num_layers=1,\n",
    "                                          norm=self.normLayer_1)\n",
    "        self.decoder_2 = TransformerDecoder(decoder_layer=self.decoderLayer_2,num_layers=1,\n",
    "                                          norm=self.normLayer_2)\n",
    "        self.decoder_3 = TransformerDecoder(decoder_layer=self.decoderLayer_3,num_layers=1,\n",
    "                                          norm=self.normLayer_3)\n",
    "        \n",
    "        self.input_embedding_smiles = nn.Embedding(dataset.vocab_size, d_model // 16)\n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(d_model // 16, dropout, max_len=enc_seq_len)\n",
    "\n",
    "        self.to_decoder_1 = nn.Conv1d(in_channels=d_model // 8, out_channels=d_model // 4, kernel_size=1)\n",
    "        self.to_decoder_2 = nn.Conv1d(in_channels=d_model // 4, out_channels=d_model // 2, kernel_size=1)\n",
    "        self.to_decoder_3 = nn.Conv1d(in_channels=d_model // 2, out_channels=d_model, kernel_size=1)\n",
    "\n",
    "    def forward(self, dec_input, latent):\n",
    "        dec_input = self.input_embedding_smiles(dec_input)\n",
    "        dec_input_0 = self.pos_encoding(dec_input)\n",
    "\n",
    "        x_mask = TriangularCausalMask(dec_input.shape[1], device=device)\n",
    "        x_mask = x_mask.bool_matrix\n",
    "        memory_mask = TriangularCausalMask(latent.shape[1], device=device)\n",
    "        memory_mask = memory_mask.bool_matrix\n",
    "\n",
    "        decoded_0 = self.decoder_0(dec_input_0, latent, tgt_mask=x_mask, memory_mask = memory_mask)\n",
    "\n",
    "        dec_input_1 = self.to_decoder_1(decoded_0.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        latent = self.to_decoder_1(latent.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        decoded_1 = self.decoder_1(dec_input_1, latent, tgt_mask=x_mask, memory_mask = memory_mask)\n",
    "\n",
    "        dec_input_2 = self.to_decoder_2(decoded_1.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        latent = self.to_decoder_2(latent.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        decoded_2 = self.decoder_2(dec_input_2, latent, tgt_mask=x_mask, memory_mask = memory_mask)\n",
    "\n",
    "        dec_input_3 = self.to_decoder_3(decoded_2.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        latent = self.to_decoder_3(latent.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        decoded_3 = self.decoder_3(dec_input_3, latent, tgt_mask=x_mask, memory_mask = memory_mask)\n",
    "\n",
    "        return decoded_3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, d_model=512, n_layers=4, n_heads=4, d_ff=32, enc_seq_len=5000,\n",
    "                 d_query=128, dropout=0.2, softmax_temp = None, attention_dropout=0.2, latent_dim = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.to_means = nn.Linear(latent_dim, latent_dim)\n",
    "        self.to_var = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "        self.encoder = TFEncoder()\n",
    "        self.decoder = TFDecoder()\n",
    "\n",
    "        self.predict = nn.Linear(d_model, dataset.vocab_size)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var).to(device)\n",
    "        eps = torch.rand_like(std).to(device)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, smiles_enc, smiles_dec_input, properties):\n",
    "\n",
    "        encoded = self.encoder(smiles_enc) # (batch_size, seq_len, d_model // 8) \n",
    "\n",
    "        means = self.to_means(encoded).permute(0, 2, 1)\n",
    "        log_var = self.to_var(encoded).permute(0, 2, 1)\n",
    "\n",
    "        z = self.reparameterize(means, log_var).permute(0, 2, 1)\n",
    "        \n",
    "        output = self.decoder(smiles_dec_input, z)\n",
    "        output = self.predict(output)\n",
    "\n",
    "        return output, means, log_var, z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(output, input, mean, log_var):\n",
    "    output = output.view(-1, dataset.vocab_size)\n",
    "    input = input.view(-1)\n",
    "    BCE = torch.nn.functional.cross_entropy(\n",
    "        output, input, reduction='sum'\n",
    "    )\n",
    "    KLD = -0.5*torch.sum(1+log_var-mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return (BCE+KLD) / input.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13, 14, 16,  0,  4,  1,  6,  6,  5, 12, 18,  0,  2,  3,  1, 16,  0,  4,\n",
      "         1, 11, 15,  0,  8,  1,  2,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "['[*;!R;O]', '[O;!R;*C]', '[CH;!R;CCO]', '(', '[CH3;!R;C]', ')', '[CH2;!R;CC]', '[CH2;!R;CC]', '[CH2;!R;CN]', '[NH;!R;CC]', '[C;!R;CNO]', '(', '=', '[O;!R;C]', ')', '[CH;!R;CCO]', '(', '[CH3;!R;C]', ')', '[O;!R;CC]', '[C;!R;*OO]', '(', '[*;!R;C]', ')', '=', '[O;!R;C]']\n",
      "*OC(C)CCCNC(=O)C(C)OC(*)=O\n"
     ]
    }
   ],
   "source": [
    "def reverse_one_hot_encoding(one_hot_tensor, vocab):\n",
    "    # 인덱스 → 토큰 매핑 생성\n",
    "    index_to_token = {idx: token for token, idx in vocab.items()}\n",
    "    \n",
    "    #print(index_to_token)\n",
    "    # 복원된 토큰 시퀀스를 저장할 리스트\n",
    "    original_tokens_list = []\n",
    "    \n",
    "    # 텐서가 GPU에 있다면 CPU로 변환\n",
    "    if one_hot_tensor.is_cuda:\n",
    "        one_hot_tensor = one_hot_tensor.cpu()\n",
    "    try:\n",
    "        one_hot_tensor.shape[1]\n",
    "        one_hot_tensor = torch.argmax(one_hot_tensor, dim=-1)\n",
    "    except:\n",
    "        one_hot_tensor.shape[0]\n",
    "\n",
    "    nonzero_indices = torch.nonzero(one_hot_tensor, as_tuple=True)[0]\n",
    "\n",
    "    try:\n",
    "        for i in range(nonzero_indices[-1]+1):\n",
    "            \n",
    "            # 인덱스를 토큰으로 변환\n",
    "            tokens = index_to_token[one_hot_tensor[i].item()]\n",
    "\n",
    "            original_tokens_list.append(tokens)\n",
    "    \n",
    "    except:\n",
    "        original_tokens_list = \"not a polymer!\"\n",
    "\n",
    "    return original_tokens_list\n",
    "print(dataset.test_data)\n",
    "test = reverse_one_hot_encoding(dataset.test_data, dataset.vocab)\n",
    "print(test)\n",
    "print(atomInSmiles.decode(' '.join(test)))\n",
    "#print(dataset.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE()\n",
    "model.cuda()\n",
    "lr = 3e-5\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "CVAE                                               [128, 264, 166]           --\n",
       "├─TFEncoder: 1-1                                   [128, 264, 64]            1,504,320\n",
       "│    └─Embedding: 2-1                              [128, 264, 256]           42,496\n",
       "│    └─PositionalEncoding: 2-2                     [128, 264, 512]           --\n",
       "│    │    └─Dropout: 3-1                           [128, 264, 512]           --\n",
       "│    └─TransformerEncoder: 2-3                     [128, 264, 512]           --\n",
       "│    │    └─ModuleList: 3-2                        --                        1,085,984\n",
       "│    │    └─LayerNorm: 3-3                         [128, 264, 512]           1,024\n",
       "│    └─Conv1d: 2-4                                 [128, 256, 264]           131,328\n",
       "│    └─TransformerEncoder: 2-5                     [128, 264, 256]           --\n",
       "│    │    └─ModuleList: 3-4                        --                        280,864\n",
       "│    │    └─LayerNorm: 3-5                         [128, 264, 256]           512\n",
       "│    └─Conv1d: 2-6                                 [128, 128, 264]           32,896\n",
       "│    └─TransformerEncoder: 2-7                     [128, 264, 128]           --\n",
       "│    │    └─ModuleList: 3-6                        --                        74,912\n",
       "│    │    └─LayerNorm: 3-7                         [128, 264, 128]           256\n",
       "│    └─Conv1d: 2-8                                 [128, 64, 264]            8,256\n",
       "│    └─TransformerEncoder: 2-9                     [128, 264, 64]            --\n",
       "│    │    └─ModuleList: 3-8                        --                        21,088\n",
       "│    │    └─LayerNorm: 3-9                         [128, 264, 64]            128\n",
       "├─Linear: 1-2                                      [128, 264, 64]            4,160\n",
       "├─Linear: 1-3                                      [128, 264, 64]            4,160\n",
       "├─TFDecoder: 1-4                                   [128, 264, 512]           2,861,248\n",
       "│    └─Embedding: 2-10                             [128, 264, 32]            5,312\n",
       "│    └─PositionalEncoding: 2-11                    [128, 264, 64]            --\n",
       "│    │    └─Dropout: 3-10                          [128, 264, 64]            --\n",
       "│    └─TransformerDecoder: 2-12                    [128, 264, 64]            --\n",
       "│    │    └─ModuleList: 3-11                       --                        37,856\n",
       "│    │    └─LayerNorm: 3-12                        [128, 264, 64]            128\n",
       "│    └─Conv1d: 2-13                                [128, 128, 264]           8,320\n",
       "│    └─Conv1d: 2-14                                [128, 128, 264]           (recursive)\n",
       "│    └─TransformerDecoder: 2-15                    [128, 264, 128]           --\n",
       "│    │    └─ModuleList: 3-13                       --                        141,216\n",
       "│    │    └─LayerNorm: 3-14                        [128, 264, 128]           256\n",
       "│    └─Conv1d: 2-16                                [128, 256, 264]           33,024\n",
       "│    └─Conv1d: 2-17                                [128, 256, 264]           (recursive)\n",
       "│    └─TransformerDecoder: 2-18                    [128, 264, 256]           --\n",
       "│    │    └─ModuleList: 3-15                       --                        544,544\n",
       "│    │    └─LayerNorm: 3-16                        [128, 264, 256]           512\n",
       "│    └─Conv1d: 2-19                                [128, 512, 264]           131,584\n",
       "│    └─Conv1d: 2-20                                [128, 512, 264]           (recursive)\n",
       "│    └─TransformerDecoder: 2-21                    [128, 264, 512]           --\n",
       "│    │    └─ModuleList: 3-17                       --                        2,137,632\n",
       "│    │    └─LayerNorm: 3-18                        [128, 264, 512]           1,024\n",
       "├─Linear: 1-5                                      [128, 264, 166]           85,158\n",
       "====================================================================================================\n",
       "Total params: 9,180,198\n",
       "Trainable params: 9,180,198\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 17.55\n",
       "====================================================================================================\n",
       "Input size (MB): 0.54\n",
       "Forward/backward pass size (MB): 3167.80\n",
       "Params size (MB): 2.50\n",
       "Estimated Total Size (MB): 3170.84\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "smiles = torch.ones([128, dataset.max_len], dtype=torch.long).to(device)\n",
    "pp = torch.ones([128, 3, 1], dtype=torch.float).to(device)\n",
    "summary(model, input_data=(smiles, smiles, pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fixed_lines(line1, line2, line3, line4, line5):\n",
    "    # 커서를 위로 3줄 올리고, 3줄을 덮어쓰기\n",
    "    sys.stdout.write(\"\\033[3F\")  # 커서를 위로 3줄 이동\n",
    "    sys.stdout.write(\"\\033[K\" + line1 + \"\\n\")  # 줄 지우고 새로 쓰기\n",
    "    sys.stdout.write(\"\\033[K\" + line2 + \"\\n\")\n",
    "    sys.stdout.write(\"\\033[K\" + line3 + \"\\n\")\n",
    "    sys.stdout.write(\"\\033[K\" + line4 + \"\\n\")\n",
    "    sys.stdout.write(\"\\033[K\" + line5 + \"\\n\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc8b7b9b243480fb2db855b7fb98d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fe13063c7f4e14add02533a04eb090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "status_out = widgets.Output()\n",
    "display(status_out)\n",
    "\n",
    "epoch = 4000\n",
    "model.train()\n",
    "progress = tqdm(range(epoch), desc=\"Training\")\n",
    "\n",
    "loss_arr = list()\n",
    "real = list()\n",
    "predict = list()\n",
    "\n",
    "for i in progress:\n",
    "    batchloss = 0.0\n",
    "    for (smiles_enc, smiles_dec_input, smiles_dec_output, properties) in train_dataloader:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        smiles_enc = smiles_enc.to(device)\n",
    "        smiles_dec_input = smiles_dec_input.to(device)\n",
    "        smiles_dec_output = smiles_dec_output.to(device)\n",
    "        properties = properties.to(device)\n",
    "\n",
    "\n",
    "        # smiles_dec_input = model.softmax(smiles_dec_input)\n",
    "        #smiles_dec_output = model.softmax(smiles_dec_output)\n",
    "\n",
    "        result, means, log_var, z = model(smiles_enc, smiles_dec_input, properties)\n",
    "        \n",
    "        loss = loss_fn(result.float(), smiles_dec_output, means, log_var)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        batchloss += loss\n",
    "    \n",
    "    loss = batchloss.cpu().item() / len(train_dataloader)\n",
    "    loss_arr.append(loss)\n",
    "    \n",
    "    #progress.set_description(\"loss: {:0.6f}\".format(loss))\n",
    "\n",
    "\n",
    "    argmax_indices = torch.argmax(result, dim=-1)\n",
    "    output = torch.nn.functional.one_hot(argmax_indices, num_classes=result.size(-1))\n",
    "\n",
    "    original_tokens = reverse_one_hot_encoding(smiles_dec_output[50], dataset.vocab)\n",
    "    predicted_tokens = reverse_one_hot_encoding(output[50], dataset.vocab)\n",
    "\n",
    "    original_str = atomInSmiles.decode(' '.join(original_tokens))\n",
    "    predicted_str = atomInSmiles.decode(' '.join(predicted_tokens))\n",
    "\n",
    "\n",
    "    # 진행 바의 속성으로부터 필요한 값들 추출 (예시)\n",
    "    elapsed = progress.format_dict.get(\"elapsed\", 0)\n",
    "    rate = progress.format_dict.get(\"rate\", None)\n",
    "    sec_per_iter = 1 / rate if rate and rate != 0 else 0\n",
    "    \n",
    "    # 고정된 2줄 상태 정보를 업데이트 (Output 위젯에 출력)\n",
    "    with status_out:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"🔹 Elapsed: {elapsed:.1f}s | sec/iter: {sec_per_iter:.3f}s\")\n",
    "        print(f\"🔹 Step: {i+1}/{progress.total}\")\n",
    "        print(\"🔹 loss: {:0.6f}\".format(loss))\n",
    "        print(f\"[Epoch {i}] Original : {original_str}\")\n",
    "        print(f\"[Epoch {i}] Predict  : {predicted_str}\")\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
