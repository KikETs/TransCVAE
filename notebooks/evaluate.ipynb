{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b816c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.widget-html-output {\n",
       "    background-color: black !important;\n",
       "    color: white !important;\n",
       "}\n",
       "div.progress-bar {\n",
       "    background-color: white !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[#C]': 0, '[#N]': 1, '[*H0]': 2, '[=*H0]': 3, '[=Branch]': 4, '[=C]': 5, '[=N]': 6, '[=O]': 7, '[=S]': 8, '[Branch]': 9, '[C]': 10, '[Cl]': 11, '[EOS]': 12, '[F]': 13, '[NH0+1]': 14, '[N]': 15, '[OH0-1]': 16, '[O]': 17, '[PAD]': 18, '[PH1]': 19, '[P]': 20, '[Ring1]': 21, '[Ring2]': 22, '[SOS]': 23, '[S]': 24, '[SiH0]': 25, '[pop]': 26}\n",
      "27\n",
      "tensor([ 2, 10, 17, 10,  2, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18])\n",
      "tensor([23,  2, 10, 17, 10,  2, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18])\n",
      "tensor([ 2, 10, 17, 10,  2, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch, rdkit\n",
    "import sys, pathlib\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path.home()/\"바탕화면\"/\"torch\"/\"Chem\"\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "from rdkit import Chem\n",
    "from utils.utils import *\n",
    "\n",
    "\n",
    "from tqdm import trange\n",
    "from pathlib import Path\n",
    "device   = \"cuda\"\n",
    "\n",
    "vocab = {'(': 0, ')': 1, '=': 2, '[O;!R;C]': 3, '[CH3;!R;C]': 4, '[CH2;!R;CN]': 5, '[CH2;!R;CC]': 6, '[CH2;!R;CO]': 7, '[*;!R;C]': 8, '[SOS]': 9, '[EOS]': 10, '[O;!R;CC]': 11, '[NH;!R;CC]': 12, '[*;!R;O]': 13, '[O;!R;*C]': 14, '[C;!R;*OO]': 15, '[CH;!R;CCO]': 16, '[CH;!R;CCN]': 17, '[C;!R;CNO]': 18, '[N;!R;CCC]': 19, '[C;!R;*NO]': 20, '[NH;!R;*C]': 21, '[*;!R;N]': 22, '[CH3;!R;N]': 23, '[CH;!R;CCC]': 24, '[C;!R;CCCO]': 25, '[CH;!R;CC]': 26, '[F;!R;C]': 27, '[CH2;!R;C]': 28, '[C;!R;COO]': 29, '[CH3;!R;O]': 30, '[C;!R;CCCN]': 31, '#': 32, '[C;!R;CCCC]': 33, '[CH2;!R;CS]': 34, '[C;!R;CC]': 35, '[OH;!R;C]': 36, '[S;!R;CC]': 37, '[N;!R;C]': 38, '[C;!R;CN]': 39, '[CH;!R;C]': 40, '[C;!R;CCC]': 41, '[NH2;!R;C]': 42, '[CH;!R;CFF]': 43, '[O;!R;S]': 44, '[O;!R;CN]': 45, '[CH;!R;CCS]': 46, '[CH2;!R;CF]': 47, '[CH3;!R;S]': 48, '[C;!R;CCO]': 49, '[NH;!R;CO]': 50, '[CH2;!R;*C]': 51, '[C;!R;NNO]': 52, '[C;!R;CFFF]': 53, '[C;!R;CCFF]': 54, '[C;!R;NOO]': 55, '[C;!R;OOO]': 56, '[S;!R;CCOO]': 57, '[CH3;!R;Si]': 58, '[C;!R;NNS]': 59, '[S;!R;C]': 60, '[NH;!R;C]': 61, '[Cl;!R;C]': 62, '[OH;!R;P]': 63, '1': 64, '[C;!R;CNN]': 65, '[CH;!R;CN]': 66, '[SH;!R;C]': 67, '[CH;!R;CCF]': 68, '[O;!R;P]': 69, '[CH2;!R;NO]': 70, '[O;R;CC]': 71, '[C;!R;CCCl]': 72, '[CH2;!R;CSi]': 73, '[CH;!R;NN]': 74, '[cH;R;CC]': 75, '[c;R;CCO]': 76, '[CH;!R;CO]': 77, '[Si;!R;CCCC]': 78, '[C;!R;CCN]': 79, '[N;!R;CN]': 80, '[NH;!R;CS]': 81, '[CH;!R;COO]': 82, '[N;!R;CCO]': 83, '[P;!R;COOO]': 84, '[C;!R;CCNN]': 85, '[N;!R;CO]': 86, '[CH;!R;CCl]': 87, '[S;!R;CNOO]': 88, '[C;!R;CCCF]': 89, '[CH2;!R;NSi]': 90, '[O;!R;CS]': 91, '[NH2;!R;O]': 92, '[NH;!R;CN]': 93, '[CH;R;CCO]': 94, '[CH2;R;CO]': 95, '[C;R;OOO]': 96, '[C;!R;CCF]': 97, '[C;!R;CCOO]': 98, '[N;!R;CCN]': 99, '[N;!R;CS]': 100, '[S;!R;CCNO]': 101, '[OH;!R;N]': 102, '[C;!R;NNN]': 103, '[S;!R;CCO]': 104, '[CH2;!R;CP]': 105, '[CH;!R;CNP]': 106, '[OH;!R;S]': 107, '[NH2;!R;N]': 108, '[N;!R;CC]': 109, '[Si;!R;*CCC]': 110, '[*;!R;Si]': 111, '[CH;!R;CS]': 112, '[C;!R;*CO]': 113, '[S;!R;COOO]': 114, '[O;!R;CP]': 115, '[S;!R;NNOO]': 116, '[NH2;!R;S]': 117, '[CH;R;*CN]': 118, '[N;R;CCC]': 119, '2': 120, '[CH2;!R;OO]': 121, '[C;!R;NN]': 122, 'N': 123, '[S;!R;COOS]': 124, '[S;!R;S]': 125, '[C;!R;CCCS]': 126, '[CH2;!R;NS]': 127, '[P;!R;OOOO]': 128, '[CH;!R;CNO]': 129, '[S;!R;OOO]': 130, '[O;!R;*S]': 131, '[CH2;!R;SS]': 132, '[CH;!R;FFO]': 133, '[CH2;!R;OP]': 134, '[C;!R;NSS]': 135, '[CH;!R;COP]': 136, '[C;!R;CFFO]': 137, '[CH;!R;CNS]': 138, '[c;R;CCC]': 139, '[CH;!R;CNN]': 140, '[C;!R;CSi]': 141, '[C;R;NNO]': 142, '[c;R;*CC]': 143, '[CH;!R;*CC]': 144, '[CH2;!R;NN]': 145, '[[N+]': 146, 'O': 147, '[[O-]': 148, '[N;!R;NO]': 149, '[CH;!R;NO]': 150, '[CH2;!R;OSi]': 151, '[CH;!R;NNN]': 152, '[P;!R;CCOO]': 153, '[C;!R;CClCl]': 154, '[C;!R;CCNO]': 155, '[C;!R;COS]': 156, '[N;!R;CCS]': 157, '[C;!R;CCCCl]': 158, '[C;!R;CCFO]': 159, '[C;!R;CCOP]': 160, '[SH;!R;O]': 161, '[S;!R;OOOO]': 162, '[C;!R;OSS]': 163, '[C;!R;CCClCl]': 164, '[PH;!R;CCO]': 165, '[PAD]': 166}\n",
    "index_to_token = {idx: token for token, idx in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebfab8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(choice, latent):\n",
    "    if choice == \"Trans_MHA\":\n",
    "        from models.Trans_MHA import CVAE, PriorNet\n",
    "        model    = CVAE(latent_dim=latent).cpu().eval()\n",
    "        model.decoder.cpu().eval\n",
    "        prior = PriorNet(y_dim=3, latent_dim=latent).cpu().eval()\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_dmodel256.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_prior.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        prior.load_state_dict(state_dict)\n",
    "\n",
    "    # Trans\n",
    "    elif choice == \"Trans\":\n",
    "        from models.Trans import CVAE, PriorNet\n",
    "        model    = CVAE(latent_dim=latent).cuda().eval()\n",
    "        model.decoder.cuda().eval()\n",
    "        prior = PriorNet(y_dim=3, latent_dim=latent).cpu().eval()\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_dmodel256_no_mha.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_prior_no_mha.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        prior.load_state_dict(state_dict)\n",
    "    # LSTM\n",
    "    elif choice == \"LSTM\":\n",
    "        from models.LSTM import CVAE, PriorNet\n",
    "        model    = CVAE(latent_dim=latent).cuda().eval()\n",
    "        model.decoder.cuda().eval()\n",
    "        prior = PriorNet(y_dim=3, latent_dim=latent).cuda().eval()\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_LSTM.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_LSTM_prior.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        prior.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "    # LSTM + MHA\n",
    "    elif choice == \"LSTM_MHA\":\n",
    "        from models.LSTM_MHA import CVAE, PriorNet\n",
    "        model    = CVAE(latent_dim=latent).cuda().eval()\n",
    "        model.decoder.cuda().eval()\n",
    "        prior = PriorNet(y_dim=3, latent_dim=latent).cuda().eval()\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_LSTM_MHA.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_LSTM_MHA_prior.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        prior.load_state_dict(state_dict)\n",
    "    \n",
    "\n",
    "    return model, prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6866acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = \"Trans\"\n",
    "latent_dim = 128\n",
    "model, prior = select_model(choice, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2ab5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09663463188245d798f0cea8501bd9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f31ea0d1ac4fd18bdf4d20badb37c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84baeaa09c54a9e971e661ada64a174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13648aa2a3f640fb9556b9b02d34c455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6309e9dac542adaa39e6d1a8963cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d224133622423b930f2f72d6b8424a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49c26c438a34f33a89b1067e3648442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float64(0.6250621103791515), 1567)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from rdkit import Chem, DataStructs\n",
    "\n",
    "#####################################################################\n",
    "# Utility: SMILES ↔ Fingerprint & Tanimoto\n",
    "#####################################################################\n",
    "\n",
    "def _smiles_to_fp(sm: str):\n",
    "    mol = Chem.MolFromSmiles(sm)\n",
    "    return Chem.RDKFingerprint(mol) if mol is not None else None\n",
    "\n",
    "\n",
    "def tanimoto(sm1: str, sm2: str) -> float:\n",
    "    fp1, fp2 = _smiles_to_fp(sm1), _smiles_to_fp(sm2)\n",
    "    if fp1 is None or fp2 is None:\n",
    "        return 0.0\n",
    "    return DataStructs.TanimotoSimilarity(fp1, fp2)\n",
    "\n",
    "#####################################################################\n",
    "# Beam‑search decoder (temperature + length‑norm)\n",
    "#####################################################################\n",
    "\n",
    "def _beam_search_decode(model, z, sos_id: int, eos_id: int, *,\n",
    "                        beam_width: int = 20, max_len: int = 128,\n",
    "                        len_penalty: float = 1.0, vocab_size: int = 9999,\n",
    "                        temperature: float = 0.9, alpha: float = 0.7):\n",
    "    \"\"\"Return list[List[int]] without SOS. OOV ids filtered.\"\"\"\n",
    "    device = z.device\n",
    "    batch  = z.size(0)\n",
    "    out_tokens = []\n",
    "    for b in tqdm(range(batch)):\n",
    "        beams = [([sos_id], 0.0, 0.0)]  # (seq, raw_logp, norm_score)\n",
    "        finished = []\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "            for seq, raw_lp, _ in beams:\n",
    "                if seq[-1] == eos_id:\n",
    "                    finished.append((seq, raw_lp))\n",
    "                    continue\n",
    "                hidden = model.decoder(\n",
    "                    torch.tensor(seq, device=device).unsqueeze(0),\n",
    "                    z[b:b+1]\n",
    "                )\n",
    "                logits = model.predict(hidden)[:, -1] / temperature  # sharpen\n",
    "                logp   = F.log_softmax(logits, dim=-1).squeeze(0)\n",
    "                topk_val, topk_idx = torch.topk(logp, k=min(beam_width*2, logp.size(0)))\n",
    "                # filter OOV\n",
    "                cand = [(tid, tv) for tid, tv in zip(topk_idx.tolist(), topk_val.tolist()) if tid < vocab_size]\n",
    "                for tid, tv in cand[:beam_width]:\n",
    "                    new_seq   = seq + [tid]\n",
    "                    new_rawlp = raw_lp + tv\n",
    "                    norm_lp   = new_rawlp / (len(new_seq) ** alpha)\n",
    "                    new_beams.append((new_seq, new_rawlp, norm_lp))\n",
    "            if not new_beams:\n",
    "                break\n",
    "            new_beams.sort(key=lambda x: x[2], reverse=True)\n",
    "            beams = new_beams[:beam_width]\n",
    "        if finished:\n",
    "            for seq, raw_lp in finished:\n",
    "                beams.append((seq, raw_lp, raw_lp / (len(seq) ** alpha)))\n",
    "        best_seq = max(beams, key=lambda x: x[1] / (len(x[0]) ** len_penalty))[0]\n",
    "        # strip SOS/EOS\n",
    "        best_seq = best_seq[1: best_seq.index(eos_id)] if eos_id in best_seq else best_seq[1:]\n",
    "        out_tokens.append(best_seq)\n",
    "    return out_tokens\n",
    "\n",
    "#####################################################################\n",
    "# Reconstruction with z = μ  (greedy or beam)\n",
    "#####################################################################\n",
    "\n",
    "def reconstruct_zmu(model, dataloader, vocab: dict, *,\n",
    "                     beam_width: int = 1, len_penalty: float = 1.0,\n",
    "                     max_len: int | None = None,\n",
    "                     temperature: float = 0.9,\n",
    "                     alpha: float = 0.7):\n",
    "    \"\"\"Return (mean_tanimoto, num_pairs). beam_width=1 → greedy.\"\"\"\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    tanis, pairs = [], 0\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    SOS, EOS, PAD = vocab['[SOS]'], vocab['[EOS]'], vocab['[PAD]']\n",
    "    if max_len is None:\n",
    "        max_len = 128\n",
    "\n",
    "    for enc_in, _, _, props in dataloader:\n",
    "        enc_in, props = enc_in.to(device), props.to(device)\n",
    "        with torch.no_grad():\n",
    "            prop_e  = model.input_embedding(props)\n",
    "            encoded = model.encoder(enc_in, prop_e)\n",
    "            mu_q    = model.to_means(encoded)\n",
    "            z       = mu_q  # z = μ\n",
    "\n",
    "            if beam_width == 1:\n",
    "                # Greedy ------------------------------------------------------\n",
    "                dec_in = torch.full((enc_in.size(0), 1), SOS, device=device)\n",
    "                done   = torch.zeros(enc_in.size(0), dtype=torch.bool, device=device)\n",
    "                out_tok= [[] for _ in range(enc_in.size(0))]\n",
    "                for _ in range(max_len):\n",
    "                    hidden = model.decoder(dec_in, z)\n",
    "                    logits = model.predict(hidden)[:, -1] / temperature\n",
    "                    next_tok = logits.argmax(-1, keepdim=True)\n",
    "                    dec_in = torch.cat([dec_in, next_tok], dim=1)\n",
    "                    for i, tok in enumerate(next_tok.squeeze(1).tolist()):\n",
    "                        if not done[i]:\n",
    "                            if tok == EOS:\n",
    "                                done[i] = True\n",
    "                            else:\n",
    "                                out_tok[i].append(tok)\n",
    "                    if done.all():\n",
    "                        break\n",
    "            else:\n",
    "                # Beam --------------------------------------------------------\n",
    "                out_tok = _beam_search_decode(model, z, SOS, EOS,\n",
    "                                              beam_width=beam_width,\n",
    "                                              max_len=max_len,\n",
    "                                              len_penalty=len_penalty,\n",
    "                                              vocab_size=len(vocab),\n",
    "                                              temperature=temperature,\n",
    "                                              alpha=alpha)\n",
    "        # SELFIES → SMILES & Tanimoto ----------------------------------------\n",
    "        for r_tok, o_tok in zip(enc_in.cpu().tolist(), out_tok):\n",
    "            ref_sm = tok_ids_to_smiles([t for t in r_tok if t not in (PAD,)])\n",
    "            out_sm = tok_ids_to_smiles([t for t in o_tok if t not in (PAD,)])\n",
    "            if ref_sm and out_sm:\n",
    "                tanis.append(tanimoto(ref_sm, out_sm))\n",
    "                pairs += 1\n",
    "    return np.mean(tanis) if tanis else 0.0, pairs\n",
    "\n",
    "\n",
    "# 1) 길이 페널티 키워서 긴 후보 선호\n",
    "print(reconstruct_zmu(model, val_dataloader, dataset.vocab,\n",
    "                      beam_width=30 , len_penalty=0.9, temperature=0.80,\n",
    "                      max_len=dataset.max_len+3))\n",
    "\n",
    "# 150/8 -> 0.4704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8919888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
