{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7224e4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.widget-html-output {\n",
       "    background-color: black !important;\n",
       "    color: white !important;\n",
       "}\n",
       "div.progress-bar {\n",
       "    background-color: white !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[#C]': 0, '[#N]': 1, '[*H0]': 2, '[=*H0]': 3, '[=Branch]': 4, '[=C]': 5, '[=N]': 6, '[=O]': 7, '[=S]': 8, '[Branch]': 9, '[C]': 10, '[Cl]': 11, '[EOS]': 12, '[F]': 13, '[NH0+1]': 14, '[N]': 15, '[OH0-1]': 16, '[O]': 17, '[PAD]': 18, '[PH1]': 19, '[P]': 20, '[Ring1]': 21, '[Ring2]': 22, '[SOS]': 23, '[S]': 24, '[SiH0]': 25, '[pop]': 26}\n",
      "27\n",
      "tensor([ 2, 10, 17, 10,  2, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18])\n",
      "tensor([23,  2, 10, 17, 10,  2, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18])\n",
      "tensor([ 2, 10, 17, 10,  2, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch, rdkit\n",
    "import sys, pathlib\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path.home()/\"바탕화면\"/\"torch\"/\"Chem\"\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "from rdkit import Chem\n",
    "from utils.utils import *\n",
    "\n",
    "\n",
    "from tqdm import trange\n",
    "from pathlib import Path\n",
    "device   = \"cuda\"\n",
    "\n",
    "vocab = dataset.vocab\n",
    "index_to_token = {idx: token for token, idx in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f5cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(choice, latent):\n",
    "    if choice == \"Trans_MHA\":\n",
    "        from models.Trans_MHA import CVAE, PriorNet\n",
    "        model    = CVAE(latent_dim=latent).cuda()\n",
    "        model.decoder.cuda()\n",
    "        prior = PriorNet(y_dim=3, latent_dim=latent).cuda()\n",
    "\n",
    "    # Trans\n",
    "    elif choice == \"Trans\":\n",
    "        from models.Trans import CVAE, PriorNet\n",
    "        model    = CVAE(latent_dim=latent).cuda()\n",
    "        model.decoder.cuda()\n",
    "        prior = PriorNet(y_dim=3, latent_dim=latent).cuda()\n",
    "\n",
    "    # LSTM\n",
    "    elif choice == \"LSTM\":\n",
    "        from models.LSTM import CVAE, PriorNet\n",
    "        model    = CVAE(latent_dim=latent).cuda()\n",
    "        model.decoder.cuda()\n",
    "        prior = PriorNet(y_dim=3, latent_dim=latent).cuda()\n",
    "\n",
    "    # LSTM + MHA\n",
    "    elif choice == \"LSTM_MHA\":\n",
    "        from models.LSTM_MHA import CVAE, PriorNet\n",
    "        model    = CVAE(latent_dim=latent).cuda()\n",
    "        model.decoder.cuda()\n",
    "        prior = PriorNet(y_dim=3, latent_dim=latent).cuda()\n",
    "\n",
    "    return model, prior\n",
    "\n",
    "def get_loss_fn(choice, latent):\n",
    "    if choice == \"Trans_MHA\":\n",
    "        loss_fn = ConditionalVAELoss(\n",
    "            vocab_size=dataset.vocab_size,\n",
    "            max_beta=0.1,\n",
    "            anneal_steps=300,\n",
    "            cyc_steps=200,\n",
    "            num_cycles=6,\n",
    "            free_bits=0.08,\n",
    "            capacity_max=0.0,\n",
    "            capacity_inc=0.0,\n",
    "            gamma=0.8,\n",
    "            prop_w=0.02,\n",
    "            nce=0.1,\n",
    "            sig_pen_p=0.003,\n",
    "            sig_pen_q=0.003,\n",
    "            imb=0.05,\n",
    "            latent_dim=latent\n",
    "        ).cuda()\n",
    "\n",
    "    # Trans\n",
    "    elif choice == \"Trans\":\n",
    "        loss_fn = ConditionalVAELoss(\n",
    "            vocab_size=dataset.vocab_size,\n",
    "            max_beta=0.1,\n",
    "            anneal_steps=300,\n",
    "            cyc_steps=200,\n",
    "            num_cycles=6,\n",
    "            free_bits=0.06,\n",
    "            capacity_max=0.0,\n",
    "            capacity_inc=0.0,\n",
    "            gamma=0.8,\n",
    "            prop_w=0.02,\n",
    "            nce=0.1,\n",
    "            sig_pen_p=0.003,\n",
    "            sig_pen_q=0.001,\n",
    "            imb=0.05,\n",
    "            latent_dim=latent\n",
    "        ).cuda()\n",
    "\n",
    "    # LSTM\n",
    "    elif choice == \"LSTM\":\n",
    "        loss_fn = ConditionalVAELoss_LSTM(\n",
    "            vocab_size=dataset.vocab_size,\n",
    "            max_beta=0.1,\n",
    "            anneal_steps=300,\n",
    "            cyc_steps=200,\n",
    "            num_cycles=6,\n",
    "            free_bits=0.08,\n",
    "            capacity_max=0.0,\n",
    "            capacity_inc=0.0,\n",
    "            gamma=0.8,\n",
    "            prop_w=0.02,\n",
    "            nce=0.1,\n",
    "            sig_pen_p=0.003,\n",
    "            sig_pen_q=0.003,\n",
    "            imb=0.05,\n",
    "            latent_dim=latent\n",
    "        ).cuda()\n",
    "\n",
    "    # LSTM + MHA\n",
    "    elif choice == \"LSTM_MHA\":\n",
    "        loss_fn = ConditionalVAELoss(\n",
    "            vocab_size=dataset.vocab_size,\n",
    "            max_beta=0.1,\n",
    "            anneal_steps=300,\n",
    "            cyc_steps=200,\n",
    "            num_cycles=6,\n",
    "            free_bits=0.08,\n",
    "            capacity_max=0.0,\n",
    "            capacity_inc=0.0,\n",
    "            gamma=0.8,\n",
    "            prop_w=0.02,\n",
    "            nce=0.1,\n",
    "            sig_pen_p=0.003,\n",
    "            sig_pen_q=0.003,\n",
    "            imb=0.05,\n",
    "            latent_dim=latent\n",
    "        ).cuda()\n",
    "        \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3caee5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"LSTM_MHA\"\n",
    "latent_dim = 128\n",
    "seq_len = dataset.max_len+3\n",
    "model, prior = select_model(mode, latent_dim)\n",
    "loss_fn = get_loss_fn(mode, latent_dim)\n",
    "\n",
    "for head in [model.to_prop, model.to_prop_z]:\n",
    "    nn.init.xavier_uniform_(head.weight)\n",
    "    nn.init.zeros_(head.bias)\n",
    "\n",
    "prop_params = list(model.to_prop.parameters())+list(model.to_prop_z.parameters())\n",
    "prop_params_id = {id(p) for p in prop_params}\n",
    "base_params = [p for p in model.parameters() if id(p) not in prop_params_id]\n",
    "lr = 3e-4\n",
    "lr_prior = lr*0.1\n",
    "from torch.optim import AdamW\n",
    "optim = AdamW([{'params':base_params, 'lr':lr},{'params':prop_params, 'lr':lr*0.5}])\n",
    "optim2 = AdamW(prior.parameters(), lr=lr_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b46e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0494a8364b4ca99a71ce16722b6f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6457b7771b214c7994b8b4c62041fd7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "from rdkit import Chem, RDLogger\n",
    "RDLogger.DisableLog('rdApp.error')\n",
    "status_out = widgets.Output()\n",
    "\n",
    "display(status_out)\n",
    "epoch = 1200\n",
    "model.train()\n",
    "prior.train()\n",
    "progress = tqdm(range(epoch), desc=\"Training\")\n",
    "loss_arr=[]\n",
    "\n",
    "for i in progress:\n",
    "    log_var_extract=[]\n",
    "    log_var_p_extract = []\n",
    "    batchloss = 0.0\n",
    "    embeddings = []\n",
    "    mean_extract = []\n",
    "    results = []\n",
    "    kld_raw_batch = 0.0\n",
    "    bce = 0.0\n",
    "    kld = 0.0\n",
    "    if mode==\"Trans\" or mode==\"Trans_MHA\":\n",
    "      for (smiles_enc, smiles_dec_input, smiles_dec_output, properties) in train_dataloader:\n",
    "        optim.zero_grad()\n",
    "        optim2.zero_grad()\n",
    "\n",
    "        smiles_enc = smiles_enc.to(device)\n",
    "        smiles_dec_input = smiles_dec_input.to(device)\n",
    "        smiles_dec_output = smiles_dec_output.to(device)\n",
    "        properties = properties.to(device)\n",
    "        output, tgt, means, log_var, tgt_z = model.forward(smiles_enc, smiles_dec_input, properties)\n",
    "    \n",
    "        mu_p, logvar_p = prior.forward(properties.squeeze())\n",
    "        with torch.no_grad():\n",
    "          log_var_extract.append(torch.exp(0.5 * log_var.cpu()).to(device))\n",
    "          log_var_p_extract.append(torch.exp(0.5 * logvar_p.cpu()).to(device))\n",
    "          mean_extract.append(means.cpu())\n",
    "        \n",
    "        loss, BCE, KLD, kld_raw, kld_per_token, prop = loss_fn.forward(output.float(),\n",
    "                                smiles_dec_output,\n",
    "                                means, log_var, mu_p, logvar_p,\n",
    "                                tgt, properties.float().squeeze(), tgt_z,\n",
    "                                i)\n",
    "        \n",
    "        results.append(output)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim2.step()\n",
    "\n",
    "        batchloss += loss.item()\n",
    "        kld_raw_batch += kld_raw.item()\n",
    "        bce += BCE.item()\n",
    "        kld += KLD.item()\n",
    "    else:\n",
    "      for (smiles_dec_input, smiles_dec_output, properties) in train_dataloader_LSTM:\n",
    "        tf_ratio = max(0.1, 1.0 - i / 300)\n",
    "        optim.zero_grad()\n",
    "        optim2.zero_grad()\n",
    "\n",
    "        smiles_dec_input = smiles_dec_input.to(device)\n",
    "        smiles_dec_output = smiles_dec_output.to(device)\n",
    "        properties = properties.to(device)\n",
    "\n",
    "        output, tgt, means, log_var, tgt_z = model.forward(smiles_dec_input, smiles_dec_output, properties, tf_ratio)\n",
    "        \n",
    "        mu_p, logvar_p = prior.forward(properties.squeeze())\n",
    "        with torch.no_grad():\n",
    "          log_var_extract.append(torch.exp(0.5 * log_var.cpu()).to(device))\n",
    "          log_var_p_extract.append(torch.exp(0.5 * logvar_p.cpu()).to(device))\n",
    "          mean_extract.append(means.cpu())\n",
    "      \n",
    "        loss, BCE, KLD, kld_raw, kld_per_token, prop = loss_fn.forward(output.float(),\n",
    "                              smiles_dec_output,\n",
    "                              means, log_var, mu_p, logvar_p,\n",
    "                              tgt, properties.float().squeeze(), tgt_z,\n",
    "                              i)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim2.step()\n",
    "\n",
    "        batchloss += loss.item()\n",
    "        kld_raw_batch += kld_raw.item()\n",
    "        bce += BCE.item()\n",
    "        kld += KLD.item()\n",
    "        \n",
    "    #Loss 값 추가\n",
    "    loss = batchloss / len(train_dataloader)\n",
    "    loss_arr.append(loss)\n",
    "\n",
    "    # #Validity\n",
    "    # results = torch.cat(results, dim=0).cpu()\n",
    "    # results = nn.functional.softmax(results, dim=-1) \n",
    "    # argmax_indices = torch.argmax(results, dim=-1)\n",
    "    \n",
    "    # valid_smiles = []\n",
    "    # for row in argmax_indices:\n",
    "    #    smiles = tok_ids_to_smiles(row.tolist())\n",
    "    #    valid_smiles.append(smiles or \"\")\n",
    "\n",
    "    # valid_count = sum(bool(s) for s in valid_smiles)\n",
    "    # valid_frac  = valid_count / len(valid_smiles)\n",
    "\n",
    "    # 진행 바의 속성으로부터 필요한 값들 추출 (예시)\n",
    "    elapsed = int(progress.format_dict.get(\"elapsed\", 0))\n",
    "    formatted_elap = str(datetime.timedelta(seconds=elapsed))\n",
    "    rate = progress.format_dict.get(\"rate\", None)\n",
    "    sec_per_iter = 1 / rate if rate and rate != 0 else 0\n",
    "    total = int(sec_per_iter * progress.total)\n",
    "    formatted_total = str(datetime.timedelta(seconds=total))\n",
    "    \n",
    "    # 고정된 상태 정보를 업데이트 (Output 위젯에 출력)\n",
    "    with status_out:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"🔹 Elapsed: {formatted_elap} | sec/iter: {sec_per_iter:.3f}s\")\n",
    "        print(\"🔹 Total time: \", formatted_total)\n",
    "        print(f\"🔹 Step: {i+1}/{progress.total}\")\n",
    "        print(\"🔹 loss: {:0.6f}\".format(loss))\n",
    "        print(\"[Posterior] sigma mean : {:0.6f}, \".format(torch.cat(log_var_extract).mean().item()),\n",
    "              \"sigma std : {:0.6f}\".format(torch.cat(log_var_extract).std().mean()))\n",
    "        print(\"[Prior]     sigma mean : {:0.6f}, \".format(torch.cat(log_var_p_extract).mean().item()),\n",
    "              \"sigma std : {:0.6f}\".format(torch.cat(log_var_p_extract).std().mean()))\n",
    "        print(\"raw KLD : \", kld_raw_batch / len(train_dataloader))\n",
    "        print(\"Posterior μ per-dim std:\", torch.cat(mean_extract,dim=0).view(-1, latent_dim).std(dim=0, unbiased=False).detach().cpu().numpy().mean())\n",
    "        print(\"Posterior μ overall std:\", torch.cat(mean_extract,dim=0).view(-1, latent_dim).std(unbiased=False).item())\n",
    "        print(\"BCE : {:0.6f},\".format(bce / seq_len / len(train_dataloader)),\n",
    "              \"KLD : {:0.6f},\".format(kld / seq_len / len(train_dataloader)),\n",
    "              \"prop : {:0.6f},\".format(prop))\n",
    "        # print(f\"Validity: {valid_frac:.2%}  ({valid_count}/{len(valid_smiles)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b5aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "prior.eval()\n",
    "results = []\n",
    "origin = []\n",
    "\n",
    "properties_results=[]\n",
    "properties_origin=[]\n",
    "print(len(val_dataset))\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        for (smiles_enc, smiles_dec_input, smiles_dec_output, properties) in val_dataloader:\n",
    "            B = smiles_enc.size(0)\n",
    "\n",
    "            smiles_enc = smiles_enc.to(device)\n",
    "            smiles_dec_input = smiles_dec_input.to(device)\n",
    "            smiles_dec_output = smiles_dec_output.to(device)\n",
    "            properties = properties.to(device)\n",
    "\n",
    "            result, tgt, means, log_var, z = model(smiles_enc, smiles_dec_input, properties)\n",
    "            # result = model.deocder(smiles_dec_input, means.view(B, -1, latent_dim))\n",
    "            # result = model.predict(result)\n",
    "\n",
    "            results.append(result)\n",
    "            origin.append(smiles_dec_output)\n",
    "\n",
    "            properties_results.append(tgt)\n",
    "            properties_origin.append(properties)\n",
    "    except:\n",
    "        for (smiles_dec_input, smiles_dec_output, properties) in val_dataloader_LSTM:\n",
    "\n",
    "            smiles_dec_input = smiles_dec_input.to(device)\n",
    "            smiles_dec_output = smiles_dec_output.to(device)\n",
    "            properties = properties.to(device)\n",
    "\n",
    "            result, tgt, means, log_var, z = model(smiles_dec_input, smiles_dec_output, properties)\n",
    "\n",
    "            results.append(result)\n",
    "            origin.append(smiles_dec_output)\n",
    "\n",
    "            properties_results.append(tgt)\n",
    "            properties_origin.append(properties)\n",
    "\n",
    "results = torch.cat(results, dim=0)\n",
    "origin = torch.cat(origin, dim=0)\n",
    "results = nn.functional.softmax(results, dim=-1) \n",
    "argmax_indices = torch.argmax(results, dim=-1)\n",
    "output = torch.nn.functional.one_hot(argmax_indices, num_classes=results.size(-1))\n",
    "print(argmax_indices)\n",
    "print(results.shape)\n",
    "print(origin.shape)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "properties_origin=torch.cat(properties_origin,dim=0)\n",
    "properties_results=torch.cat(properties_results,dim=0)\n",
    "MAE_2 = mean_absolute_error(properties_origin.squeeze().cpu(), properties_results.squeeze().cpu())\n",
    "print(\"MAE(properties) : \", MAE_2)\n",
    "\n",
    "\n",
    "results_smiles = []\n",
    "origin_smiles = []\n",
    "\n",
    "for row in argmax_indices:\n",
    "    smiles = tok_ids_to_smiles(row.tolist())\n",
    "    results_smiles.append(smiles or \"\")\n",
    "\n",
    "for row in origin:\n",
    "    smiles = tok_ids_to_smiles(row.tolist())\n",
    "    origin_smiles.append(smiles or \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b780578",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_smiles = [smiles.removesuffix(\"EOS\").strip() for smiles in origin_smiles]\n",
    "results_smiles = [smiles.removesuffix(\"EOS\").strip() for smiles in results_smiles]\n",
    "\n",
    "for i in range(len(results_smiles)):\n",
    "    if(origin_smiles[i] != results_smiles[i]):\n",
    "        print(i, \"번째 다름!\")\n",
    "    print(\"real smiles      : \", origin_smiles[i])\n",
    "    print(\"predicted smiles : \", results_smiles[i])\n",
    "\n",
    "\n",
    "MAE = mean_absolute_error(origin.cpu(), torch.argmax(results.cpu(), dim=-1))\n",
    "print(\"MAE : \", MAE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import DataStructs, rdFingerprintGenerator\n",
    "RDLogger.DisableLog('rdApp.error')\n",
    "\n",
    "generator = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "\n",
    "def tanimoto_similarity(smiles1, smiles2):\n",
    "    mol1 = Chem.MolFromSmiles(smiles1)\n",
    "    mol2 = Chem.MolFromSmiles(smiles2)\n",
    "    fp1 = generator.GetFingerprint(mol1)\n",
    "    fp2 = generator.GetFingerprint(mol2)\n",
    "    return DataStructs.TanimotoSimilarity(fp1, fp2)\n",
    "\n",
    "def is_valid(smiles):\n",
    "    return Chem.MolFromSmiles(smiles) is not None\n",
    "\n",
    "TS = 0.0\n",
    "canbe = 0\n",
    "notbe = 0\n",
    "\n",
    "for sm, orig in zip(results_smiles, origin_smiles):\n",
    "    if(len(sm)==0):\n",
    "        notbe += 1\n",
    "        continue\n",
    "    if is_valid(sm) and is_valid(orig):\n",
    "        sim = tanimoto_similarity(sm, orig)\n",
    "        TS += sim\n",
    "        canbe += 1\n",
    "    else:\n",
    "        notbe += 1\n",
    "\n",
    "if canbe > 0:\n",
    "    print(\"Tanimoto Similarity : \", TS / canbe)\n",
    "else:\n",
    "    print(\"No valid molecules to compare.\")\n",
    "\n",
    "print(\"가능한 분자 개수 :\", canbe)\n",
    "print(\"불가능한 분자 개수 :\", notbe)\n",
    "print(\"Valid fraction      :\", canbe / len(results_smiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f346af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(choice):\n",
    "    if choice == \"Trans_MHA\":\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_dmodel256.pth\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_prior.pth\")        \n",
    "        torch.save(prior.state_dict(), save_path)\n",
    "\n",
    "    # Trans\n",
    "    elif choice == \"Trans\":\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_dmodel256_no_mha.pth\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_prior_no_mha.pth\")        \n",
    "        torch.save(prior.state_dict(), save_path)\n",
    "    # LSTM\n",
    "    elif choice == \"LSTM\":\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_LSTM.pth\")        \n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_LSTM_prior.pth\")       \n",
    "        torch.save(prior.state_dict(), save_path)\n",
    "\n",
    "\n",
    "    # LSTM + MHA\n",
    "    elif choice == \"LSTM_MHA\":\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_LSTM_MHA.pth\")        \n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_LSTM_MHA_prior.pth\")        \n",
    "        torch.save(prior.state_dict(), save_path)\n",
    "\n",
    "def select_model(choice, latent):\n",
    "    if choice == \"Trans_MHA\":\n",
    "        from models.Trans_MHA import CVAE, PriorNet\n",
    "        model    = CVAE(latent_dim=latent).cuda().eval()\n",
    "        model.decoder.cuda().eval\n",
    "        prior = PriorNet(y_dim=3, latent_dim=latent).cuda().eval()\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_dmodel256.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_prior.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        prior.load_state_dict(state_dict)\n",
    "\n",
    "    # Trans\n",
    "    elif choice == \"Trans\":\n",
    "        from models.Trans import CVAE, PriorNet\n",
    "        model    = CVAE(latent_dim=latent).cuda().eval()\n",
    "        model.decoder.cuda().eval()\n",
    "        prior = PriorNet(y_dim=3, latent_dim=latent).cuda().eval()\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_dmodel256_no_mha.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_prior_no_mha.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        prior.load_state_dict(state_dict)\n",
    "    # LSTM\n",
    "    elif choice == \"LSTM\":\n",
    "        from models.LSTM import CVAE, PriorNet\n",
    "        model    = CVAE(latent_dim=latent).cuda().eval()\n",
    "        model.decoder.cuda().eval()\n",
    "        prior = PriorNet(y_dim=3, latent_dim=latent).cuda().eval()\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_LSTM.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_LSTM_prior.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        prior.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "    # LSTM + MHA\n",
    "    elif choice == \"LSTM_MHA\":\n",
    "        from models.LSTM_MHA import CVAE, PriorNet\n",
    "        model    = CVAE(latent_dim=latent).cuda().eval()\n",
    "        model.decoder.cuda().eval()\n",
    "        prior = PriorNet(y_dim=3, latent_dim=latent).cuda().eval()\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_LSTM_MHA.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        save_path = (PROJECT_ROOT / \"models/weights\" / \"model_weights_LSTM_MHA_prior.pth\")\n",
    "        state_dict = torch.load(save_path)\n",
    "        prior.load_state_dict(state_dict)\n",
    "    \n",
    "\n",
    "    return model, prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3005174",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_weights(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d662bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from rdkit import Chem, DataStructs\n",
    "\n",
    "#####################################################################\n",
    "# Utility: SMILES ↔ Fingerprint & Tanimoto\n",
    "#####################################################################\n",
    "\n",
    "def _smiles_to_fp(sm: str):\n",
    "    mol = Chem.MolFromSmiles(sm)\n",
    "    return Chem.RDKFingerprint(mol) if mol is not None else None\n",
    "\n",
    "\n",
    "def tanimoto(sm1: str, sm2: str) -> float:\n",
    "    fp1, fp2 = _smiles_to_fp(sm1), _smiles_to_fp(sm2)\n",
    "    if fp1 is None or fp2 is None:\n",
    "        return 0.0\n",
    "    return DataStructs.TanimotoSimilarity(fp1, fp2)\n",
    "\n",
    "#####################################################################\n",
    "# Beam‑search decoder (temperature + length‑norm)\n",
    "#####################################################################\n",
    "\n",
    "def _beam_search_decode(model, z, sos_id: int, eos_id: int, *,\n",
    "                        beam_width: int = 20, max_len: int = 128,\n",
    "                        len_penalty: float = 1.0, vocab_size: int = 9999,\n",
    "                        temperature: float = 0.9, alpha: float = 0.7):\n",
    "    \"\"\"Return list[List[int]] without SOS. OOV ids filtered.\"\"\"\n",
    "    device = z.device\n",
    "    batch  = z.size(0)\n",
    "    out_tokens = []\n",
    "    for b in tqdm(range(batch)):\n",
    "        beams = [([sos_id], 0.0, 0.0)]  # (seq, raw_logp, norm_score)\n",
    "        finished = []\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "            for seq, raw_lp, _ in beams:\n",
    "                if seq[-1] == eos_id:\n",
    "                    finished.append((seq, raw_lp))\n",
    "                    continue\n",
    "                hidden = model.decoder(\n",
    "                    torch.tensor(seq, device=device).unsqueeze(0),\n",
    "                    z[b:b+1]\n",
    "                )\n",
    "                logits = model.predict(hidden)[:, -1] / temperature  # sharpen\n",
    "                logp   = F.log_softmax(logits, dim=-1).squeeze(0)\n",
    "                topk_val, topk_idx = torch.topk(logp, k=min(beam_width*2, logp.size(0)))\n",
    "                # filter OOV\n",
    "                cand = [(tid, tv) for tid, tv in zip(topk_idx.tolist(), topk_val.tolist()) if tid < vocab_size]\n",
    "                for tid, tv in cand[:beam_width]:\n",
    "                    new_seq   = seq + [tid]\n",
    "                    new_rawlp = raw_lp + tv\n",
    "                    norm_lp   = new_rawlp / (len(new_seq) ** alpha)\n",
    "                    new_beams.append((new_seq, new_rawlp, norm_lp))\n",
    "            if not new_beams:\n",
    "                break\n",
    "            new_beams.sort(key=lambda x: x[2], reverse=True)\n",
    "            beams = new_beams[:beam_width]\n",
    "        if finished:\n",
    "            for seq, raw_lp in finished:\n",
    "                beams.append((seq, raw_lp, raw_lp / (len(seq) ** alpha)))\n",
    "        best_seq = max(beams, key=lambda x: x[1] / (len(x[0]) ** len_penalty))[0]\n",
    "        # strip SOS/EOS\n",
    "        best_seq = best_seq[1: best_seq.index(eos_id)] if eos_id in best_seq else best_seq[1:]\n",
    "        out_tokens.append(best_seq)\n",
    "    return out_tokens\n",
    "\n",
    "#####################################################################\n",
    "# Reconstruction with z = μ  (greedy or beam)\n",
    "#####################################################################\n",
    "\n",
    "def reconstruct_zmu(model, dataloader, vocab: dict, *,\n",
    "                     beam_width: int = 1, len_penalty: float = 1.0,\n",
    "                     max_len: int | None = None,\n",
    "                     temperature: float = 0.9,\n",
    "                     alpha: float = 0.7):\n",
    "    \"\"\"Return (mean_tanimoto, num_pairs). beam_width=1 → greedy.\"\"\"\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    tanis, pairs = [], 0\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    SOS, EOS, PAD = vocab['[SOS]'], vocab['[EOS]'], vocab['[PAD]']\n",
    "    if max_len is None:\n",
    "        max_len = 128\n",
    "\n",
    "    for enc_in, _, _, props in dataloader:\n",
    "        enc_in, props = enc_in.to(device), props.to(device)\n",
    "        with torch.no_grad():\n",
    "            prop_e  = model.input_embedding(props)\n",
    "            encoded = model.encoder(enc_in, prop_e)\n",
    "            mu_q    = model.to_means(encoded)\n",
    "            z       = mu_q  # z = μ\n",
    "\n",
    "            if beam_width == 1:\n",
    "                # Greedy ------------------------------------------------------\n",
    "                dec_in = torch.full((enc_in.size(0), 1), SOS, device=device)\n",
    "                done   = torch.zeros(enc_in.size(0), dtype=torch.bool, device=device)\n",
    "                out_tok= [[] for _ in range(enc_in.size(0))]\n",
    "                for _ in range(max_len):\n",
    "                    hidden = model.decoder(dec_in, z)\n",
    "                    logits = model.predict(hidden)[:, -1] / temperature\n",
    "                    next_tok = logits.argmax(-1, keepdim=True)\n",
    "                    dec_in = torch.cat([dec_in, next_tok], dim=1)\n",
    "                    for i, tok in enumerate(next_tok.squeeze(1).tolist()):\n",
    "                        if not done[i]:\n",
    "                            if tok == EOS:\n",
    "                                done[i] = True\n",
    "                            else:\n",
    "                                out_tok[i].append(tok)\n",
    "                    if done.all():\n",
    "                        break\n",
    "            else:\n",
    "                # Beam --------------------------------------------------------\n",
    "                out_tok = _beam_search_decode(model, z, SOS, EOS,\n",
    "                                              beam_width=beam_width,\n",
    "                                              max_len=max_len,\n",
    "                                              len_penalty=len_penalty,\n",
    "                                              vocab_size=len(vocab),\n",
    "                                              temperature=temperature,\n",
    "                                              alpha=alpha)\n",
    "        # SELFIES → SMILES & Tanimoto ----------------------------------------\n",
    "        for r_tok, o_tok in zip(enc_in.cpu().tolist(), out_tok):\n",
    "            ref_sm = tok_ids_to_smiles([t for t in r_tok if t not in (PAD,)])\n",
    "            out_sm = tok_ids_to_smiles([t for t in o_tok if t not in (PAD,)])\n",
    "            if ref_sm and out_sm:\n",
    "                tanis.append(tanimoto(ref_sm, out_sm))\n",
    "                pairs += 1\n",
    "    return np.mean(tanis) if tanis else 0.0, pairs\n",
    "\n",
    "\n",
    "# 1) 길이 페널티 키워서 긴 후보 선호\n",
    "print(reconstruct_zmu(model, val_dataloader, dataset.vocab,\n",
    "                      beam_width=100, len_penalty=1.0, temperature=0.80,\n",
    "                      max_len=dataset.max_len+3))\n",
    "\n",
    "# 150/8 -> 0.4704"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
